# -*- coding: utf-8 -*-
"""project .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1229XB9F8FqyKelPU1OuYs6FCmoa84Upq

**Loan Approval Prediction using Machine Learning**
"""

from google.colab import drive
drive.mount('/content/drive/')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn import metrics
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression
from imblearn.over_sampling import RandomOverSampler

import warnings
warnings.filterwarnings('ignore')

path_loan = '/content/drive/MyDrive/Projects/LoanApprovalPrediction.csv'

data = pd.read_csv(path_loan)

data.head(5)

"""**Data Preprocessing and Visualization**"""

data.info()

data.shape

# Some Statistical Features
data.describe()

obj = (data.dtypes == 'object')
print("Categorical variables:",len(list(obj[obj].index)))

# As Loan_ID is completely unique and not correlated with any of the other column, So we will drop it using .drop() function

# Dropping Loan_ID column
data.drop(['Loan_ID'],axis=1,inplace=True)

obj = (data.dtypes == 'object')
object_cols = list(obj[obj].index)
plt.figure(figsize=(18,36))
index = 1

for col in object_cols:
  y = data[col].value_counts()
  plt.subplot(11,4,index)
  plt.xticks(rotation=90)
  sns.barplot(x=list(y.index), y=y)
  index +=1

# Import label encoder
from sklearn import preprocessing

# label_encoder object knows how
# to understand word labels.
label_encoder = preprocessing.LabelEncoder()
obj = (data.dtypes == 'object')
for col in list(obj[obj].index):
  data[col] = label_encoder.fit_transform(data[col])

# To find the number of columns with
# datatype==object
obj = (data.dtypes == 'object')
print("Categorical variables:",len(list(obj[obj].index)))

# Heatmap
plt.figure(figsize=(12,6))

sns.heatmap(data.corr(),cmap='BrBG',fmt='.2f',
			linewidths=2,annot=True)

# Catplot to visualize the plot for the Gender, and Marital Status of the applicant.
sns.catplot(x="Gender", y="Married",
            hue="Loan_Status",
            kind="bar",
            data=data)

# find out if there is any missing values in the dataset
for col in data.columns:
 data[col] = data[col].fillna(data[col].mean())

data.isna().sum()

"""Model training"""

X = data.drop(['Loan_Status'],axis=1)

X

Y = data['Loan_Status'].values

Y

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
x_train, x_test, y_train, y_test = train_test_split(data.values, Y, test_size=0.2)

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

#KNeighborsClassifiers
#RandomForestClassifiers
#Support Vector Classifiers (SVC)
#Logistics Regression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression

from sklearn import metrics

knn = KNeighborsClassifier(n_neighbors=3)
rfc = RandomForestClassifier(n_estimators = 7,
							criterion = 'entropy',
							random_state =7)
svc = SVC()
lc = LogisticRegression()

# making predictions on the training set
for clf in (rfc, knn, svc,lc):
	clf.fit(x_train, y_train)
	Y_pred = clf.predict(x_train)
	print("Accuracy score of ",
		clf.__class__.__name__,
		"=",100*metrics.accuracy_score(y_train,
										Y_pred))

# making predictions on the testing set
for clf in (rfc, knn, svc,lc):
	clf.fit(x_train, y_train)
	Y_pred = clf.predict(x_test)
	print("Accuracy score of ",
		clf.__class__.__name__,"=",
		100*metrics.accuracy_score(y_test,
									Y_pred))